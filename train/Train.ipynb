{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Keras Image Classifier\n",
    "Welcome to an example model training notebook classifying different dog breeds. Hopefully this will be suffficently documented but be sure to further reasearch terms to get a better understanding and better optimise it for your use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords\n",
    "<b>Before we get started</b> it's really important to understand some essential key words. They come up time and time again in machine learning and are essential for understanding what's happening and how to improve your designs.\n",
    "<ul>\n",
    "    <li><b style=\"font-size:20px;\">Neural net (model):</b> Arguably the most important term to know. This is the artificial element that processes data and makes decisions. It can be thought of in a similar way to a biological neural network which refers to how the brain processes inputs through neurons to create responses. In machine learning we design models which outline the structure for neural nets and then through training, optimises its structure and connections to make it good at whatevers its been designed to do.</li>\n",
    "    <li><b style=\"font-size:20px;\">Overfitting:</b> Regards to models that are fitted too tightly to their training data, and are poor at adapting to unseen data. This is an important phenomenon to avoid as although you'll get a high accuracy score in training, your model will actually adapt poorly when used in reality. To avoid this; \n",
    "        <ul>\n",
    "            <li><b>Diverse data:</b> Make sure you dataset is large and diverse. If your images have unrelated features that occur across the class (like all huskys are photographed in the snow) your model may link those features to what it determines a class by, and then not respond correctly in other cases. Large datasets are essential.</li>\n",
    "            <li><b>Dropout:</b> One of the key structures in CNN's play a vital role in its design and helps reduce overfitting by removing connections to make the model more flexible</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Imports</h2>\n",
    "<ul>\n",
    "    <li><b>Keras:</b> The main library of use in this project. It's a machine learning library useful for designing neural nets and in this implementation works on top of tensorflow (google's machine learning library)</li>\n",
    "    <li><b>Scikit-learn (sklearn):</b> Supports some of the mathmatical operations needed and provides helpful tools</li>\n",
    "    <li><b>NumPy:</b> Provides array and mathmatical strucutres to python</li>\n",
    "    <li><b>pickle:</b> Used for serialising (encoding) objects to save to disk</li>\n",
    "    <li><b>opencv (cv2):</b> Computer vision library used for image manipulation</li>\n",
    "    <li><b>os:</b> Standard python library for checking file directories</li>\n",
    "    <li><b>tqdm:</b> Library that prints out loop progress (optional if removed from used cells)</li>\n",
    "</ul>  \n",
    "You'll also note <b>neuralNetStructure</b>. This is a separate python file containing the neural net design used in our model. This can be modified to help improve your system, but make sure your understand the structures involved. Read through it when you've been through and understand this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-343bf7aa03af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# import the necessary packages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from neuralNetStructure import NeuralNetStructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Important variables</h2>\n",
    "<ul>\n",
    "    <li><b>Epochs:</b> See definition above</li>\n",
    "    <li><b>Initial learning rate (init_lr):</b> The amount weights are stepped (updated) during training.</li>\n",
    "    <li><b>Batch size (bs):</b> Number of sampls used per iteration</li>\n",
    "    <li><b>Image dimensions (image_dims):</b> Refers to the structure each image will be converted to with the values mapping to width, height and depth. Depth refers to the colour of the image with 3 being rgb colour (red,green,blue = 3) but may also sometimes be 1 (grayscale) as this is not important to the model</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "INIT_LR = 1e-3\n",
    "BS = 32\n",
    "IMAGE_DIMS = (96, 96, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the folder containing your image folders in the <b>dataFolder</b> variable.<br>\n",
    "<b>data</b> is the array which will hold the images converted to array format. <br>\n",
    "<b>labels</b> is the array holding the corresponding class name (breed) for each of these images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFolder = \"dataset\"\n",
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Converting images</h2>\n",
    "<p>Computers don't understand images but they do understand numbers. Before a machine learning system can work with images it must convert them into their numeric format (values of each pixel). They also need to be standardaised into the same format to be fed into the model</p>\n",
    "<ol>\n",
    "    <li>The <b>outer for loop</b> iterates through each folder with the <b>inner loop</b> iterating over each image in the current folder</li>\n",
    "    <li>Each image is firstly loaded into the image variable by <b>opencv (cv2)</b> which reads the image in the current image path (folder and image name)</li>\n",
    "    <li>Then the image is <b>resized</b> into the width and height stated in <b>IMAGE_DIMS</b></li>\n",
    "    <li>Then the image converted into an array (of pixel values) and added to the <b>data</b> list</li>\n",
    "    <li>The class name is also appended to the <b>labels</b> list (name of the folder)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.87it/s]\n"
     ]
    }
   ],
   "source": [
    "for folder in tqdm.tqdm(os.listdir(dataFolder)):\n",
    "    for img in os.listdir(dataFolder+\"/\"+folder):\n",
    "        # load the image, pre-process it, and store it in the data list\n",
    "        imagePath = dataFolder+\"/\"+folder+\"/\"+img\n",
    "        image = cv2.imread(imagePath)\n",
    "        image = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "        image = img_to_array(image)\n",
    "        data.append(image)\n",
    "\n",
    "        # save folder name as label\n",
    "        labels.append([folder])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The data and labels lists are converted to numpy arrays as these are the preferred format for our libraries and are better optimsed for purpose. the data array is also converted to a <b>float</b> and divided by <b>255</b> to scale the pixel values to a range of 0 to 1</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data matrix: 269 images (58.10MB)\n",
      "[['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Chihuahuas']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Bulldogs']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']\n",
      " ['Beagles']]\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "print(\"[INFO] data matrix: {} images ({:.2f}MB)\".format(\n",
    "    len(labels), data.nbytes / (1024 * 1000.0)))\n",
    "print(labels[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>At this stage our labels are correct but the machine learning model needs them to be optimised for purpose. This process called <b>One hot encoding</b> helps by converting category names into numeric (binary) representation to work with prediction. Importantly each class name still maps to their numeric representation so we can still link predicitons back to the original classes</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Beagles\n",
      "2. Bulldogs\n",
      "3. Chihuahuas\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(labels)\n",
    "for (i, label) in enumerate(mlb.classes_):\n",
    "    print(\"{}. {}\".format(i + 1, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data manipulation</h2>\n",
    "<p>Although our labels and images have now been converted into numeric format and optimised for our model we still need to do more. Firstly we need to split into training and testing data (explained below)  and then we can make an image augmentor to further our image data</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train & Test Data</h3>\n",
    "<p>The idea of training and testing data is essential to machine learning. <b>Training data</b> is what the model analyses to manipulate its structure to fit around. Although this may seem to be the only thing we need to do in training, we also need <b>Testing data</b> to validate its performance. Testing data is explicitly left out of what the model fits around to assess the models performance on. This helps generalise the model and links to the concept of avoiding <b>overfitting</b> (see definitions)</p>\n",
    "<p>In the above cell we use the <b>train_test_split</b> function to return our data (images) and labels into <b>4</b> new lists. The 'X' lists contains the image data, whilst the 'Y' lists contains the labels (order maintained). </p>\n",
    "<p>The reason these are regarded to as X and Y is to do with how machine learning prediction is conceptualised, which is typically with a graph. As we know these refer to the two axis graph have, with the x value determining y's. This works the same here with the x data (image) driving the value of y (class label).</p>\n",
    "<p>The other important argument to note is <b>test_size</b> which defines what percentage of the data should be used for testing during training. This value is typically in the range of 20% (80:20 split) to 33% (67:33 split). The tradeoff to this value is the more you put into testing the less data the model has to train on, but having a small test size will also increase overfitting. <b>random_state</b> simply defines a seed for random to decide how to randomly sort data and is optional</p>\n",
    "<p>After this cell runs you're data has been divided between training and testing and is also still split into X (images) and Y (classes) hence 4 arrays</p>\n",
    "<ul>\n",
    "    <li>trainX: training images</li>\n",
    "    <li>testX: testing images</li>\n",
    "    <li>trainY: training classes</li>\n",
    "    <li>testY: testing labels</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Image augmentation</h3>\n",
    "<p>As we've disussed diverse data is essential to a project, but that doesn't have to end at the dataset building stage. With an augmentor we can get more from our data by manipulating it to account for different angles, translations and rotations. This is another important tool for avoiding <b>overfitting</b> and generalising a model. For example if our dog images mostly showed the dog facing to the right, it may do poorly with an image of a dog facing left. Using the augmentor we can simulate that same image flipped to overcome this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>\n",
    "<p>Here we are. The stage where we finally put our processed data through the system. A lot of keywords are used here so look at the keywords at the top for help and reasearch more to improve your design.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Firstly we need to load our model. Here we are calling the <b>build</b> function in our <b>NeuralNetStructure</b> class (see that file for how it's designed). We pass it the image width, height and depth of the images as well as how many classes we're training for. We also declare the <b>activation function</b> to be <b>sigmoid</b>. This is a widely used function that will map any returned values to betweeen 1 and 0 (hence used for outputting probability). This function will return the model structure we will train with</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetStructure.build(\n",
    "    width=IMAGE_DIMS[1], height=IMAGE_DIMS[0],\n",
    "    depth=IMAGE_DIMS[2], classes=len(mlb.classes_),\n",
    "    finalAct=\"sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We also need to declare an optimiser. This is an object which is used by the model to evaluate how it's doing and tweak values to improve. They use a loss function to establish what weights should be changed over time for the best results. I've gone with Adam which is a widely used opotimiser but please research for yourself to see what other optimisers may offer. Here we pass the <b>learning rate</b> (explained in code cell 2) and <b>decay</b> which is how much the learning rate reduces by each epoch (once again please research more about these if interested) </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The last step before training is to bring the components together. We compile the model with the optimiser. We also ... </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "6/6 [==============================] - 10s 2s/step - loss: 1.0945 - acc: 0.5365 - val_loss: 1.2503 - val_acc: 0.5247\n",
      "Epoch 2/15\n",
      "6/6 [==============================] - 7s 1s/step - loss: 1.0905 - acc: 0.5828 - val_loss: 1.3579 - val_acc: 0.5247\n",
      "Epoch 3/15\n",
      "6/6 [==============================] - 7s 1s/step - loss: 0.8289 - acc: 0.6375 - val_loss: 1.4148 - val_acc: 0.5123\n",
      "Epoch 4/15\n",
      "6/6 [==============================] - 7s 1s/step - loss: 0.7511 - acc: 0.6399 - val_loss: 1.1116 - val_acc: 0.5432\n",
      "Epoch 5/15\n",
      "6/6 [==============================] - 7s 1s/step - loss: 0.7545 - acc: 0.6439 - val_loss: 0.9678 - val_acc: 0.5741\n",
      "Epoch 6/15\n",
      "6/6 [==============================] - 7s 1s/step - loss: 0.6736 - acc: 0.6667 - val_loss: 1.1474 - val_acc: 0.5988\n",
      "Epoch 7/15\n",
      "6/6 [==============================] - 7s 1s/step - loss: 0.6335 - acc: 0.6882 - val_loss: 0.9378 - val_acc: 0.5926\n",
      "Epoch 8/15\n",
      "6/6 [==============================] - 8s 1s/step - loss: 0.6682 - acc: 0.6911 - val_loss: 0.7986 - val_acc: 0.6296\n",
      "Epoch 9/15\n",
      "6/6 [==============================] - 7s 1s/step - loss: 0.6207 - acc: 0.7205 - val_loss: 0.7446 - val_acc: 0.6049\n",
      "Epoch 10/15\n",
      "6/6 [==============================] - 6s 1s/step - loss: 0.6338 - acc: 0.7098 - val_loss: 0.7702 - val_acc: 0.6296\n",
      "Epoch 11/15\n",
      "6/6 [==============================] - 7s 1s/step - loss: 0.5506 - acc: 0.7383 - val_loss: 0.6682 - val_acc: 0.6790\n",
      "Epoch 12/15\n",
      "6/6 [==============================] - 7s 1s/step - loss: 0.5811 - acc: 0.7153 - val_loss: 0.5813 - val_acc: 0.7160\n",
      "Epoch 13/15\n",
      "6/6 [==============================] - 7s 1s/step - loss: 0.6185 - acc: 0.7145 - val_loss: 0.6604 - val_acc: 0.6420\n",
      "Epoch 14/15\n",
      "6/6 [==============================] - 6s 1s/step - loss: 0.5793 - acc: 0.7191 - val_loss: 0.7612 - val_acc: 0.6420\n",
      "Epoch 15/15\n",
      "6/6 [==============================] - 6s 1s/step - loss: 0.5670 - acc: 0.7453 - val_loss: 0.8090 - val_acc: 0.6049\n"
     ]
    }
   ],
   "source": [
    "H = model.fit_generator(\n",
    "    aug.flow(trainX, trainY, batch_size=BS),\n",
    "    validation_data=(testX, testY),\n",
    "    steps_per_epoch=len(trainX) // BS,\n",
    "    epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"label.pickle\", \"wb\")\n",
    "f.write(pickle.dumps(mlb))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
